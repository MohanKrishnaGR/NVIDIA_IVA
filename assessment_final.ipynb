{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5101d226-325d-407a-82d7-5b221da9ce69",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"><img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53007a3d-38ff-4cd1-859b-0ced1e8500cd",
   "metadata": {},
   "source": [
    "## Assessment: Building a Real-Time Video AI Application ##\n",
    "In this notebook, you will utilize what you've learned in this course to complete an assessment. The assessment has been divided into a couple of steps - each of which will generate a text file for grading purposes. You will be graded based on the following rubric. Note that this coding portion does not give partial credit - it shows up as either 0 or 60 points. Earning 50 points or more will award you the full 60 points, while earning less than 50 points will award you 0 points for the coding portion. \n",
    "<table border=\"1\" class=\"dataframe\" align='left'>  <thead>    <tr style=\"text-align: right;\">      <th>Step</th>      <th># of &lt;FIXME&gt;</th>      <th>Points</th>    </tr>  </thead>  <tbody>    <tr>      <td>0. The Problem</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <td>1. Understanding the Input Video</td>      <td>5</td>      <td>10</td>    </tr>    <tr>      <td>2. Brainstorm AI Inference and Download a Pre-Trained Model</td>      <td>2</td>      <td>10</td>    </tr>    <tr>      <td>3. Edit the Inference Configuration File</td>      <td>10</td>      <td>10</td>    </tr>    <tr>      <td>4. Build and Run DeepStream Pipeline</td>      <td>20</td>      <td>20</td>    </tr>    <tr>      <td>5. Analyze the Results</td>      <td>1</td>      <td>10</td>    </tr>    <tr>      <td>BONUS. Visualize Frames</td>      <td>0</td>      <td>0</td>    </tr>  </tbody></table>\n",
    "\n",
    "<p><img src='images/iva_framework.png' width=600></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea28a7a-3536-4c75-8c84-5978f49db9a6",
   "metadata": {},
   "source": [
    "### Step 0: The Problem ###\n",
    "You are a developer for an automobile fleet management company. You have recently installed dashboard cameras on all of the vehicles and are ready to implement AI to analyze the fleet's driving behavior. One of the issues you've noticed with the fleet is [tailgating](https://en.wikipedia.org/wiki/Tailgating), which occurs when the vehicle drives behind another vehicle without leaving sufficient distance to stop without causing a collision if the vehicle in front stops suddenly. You've decied to build a DeepStream application that will help monitor this behavior. At this point, you want to be able to log occurences of tailgating so you can understand the frequency. Note that while the input video sources are static files, the pipeline can easily be modified to consume videos in real-time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f1c9bd-488a-4b54-bbc0-80ab4ecd5c4e",
   "metadata": {},
   "source": [
    "**Instructions**: <br>\n",
    "0.1 Execute the cell to set the target video as an environment variable. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f79d8c-9bc3-4f2b-929b-3baab61cbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import os\n",
    "os.environ['TARGET_VIDEO_PATH']='data/assessment_stream.h264'\n",
    "os.environ['TARGET_VIDEO_PATH_MP4']='data/assessment_stream.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d18bdd-6ee6-4241-aa2b-d057bd04309d",
   "metadata": {},
   "source": [
    "### Step 1: Understanding the Input Video ###\n",
    "The first step is to understand the properties of the input videos before we can design a system to digest them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42feee71-f5d3-4f15-a120-5aaf12724b70",
   "metadata": {},
   "source": [
    "Use the `ffprobe` ([see documentation if needed](https://ffmpeg.org/ffprobe.html)) command line utility to obtain the `height`, `width`, and `frame rate` of the input video. We're also using the `-hide_banner` option to minimize the text output. \n",
    "\n",
    "**Instructions**: <br>\n",
    "1.1 Execute the cell to preview the video. <br>\n",
    "1.2 Execute the cell to gather information from input video stream. <br>\n",
    "1.3 Modify the `<FIXME>`s _only_ to the correct values and execute the cell to mark your answer. _You can execute this cell multiple times until satisfactory_. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4976ef4-5a16-4106-8454-48c07cfad8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"data/assessment_stream.mp4\" controls  width=\"720\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1\n",
    "# DO NOT CHANGE THIS CELL\n",
    "from IPython.display import Video\n",
    "Video(os.environ['TARGET_VIDEO_PATH_MP4'], width=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83672ee2-3e0f-47b1-bac3-f22df24669e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input #0, h264, from 'data/assessment_stream.h264':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "    Stream #0:0: Video: h264 (High), yuv420p(progressive), 1280x720, 59.94 fps, 59.94 tbr, 1200k tbn, 119.88 tbc\n"
     ]
    }
   ],
   "source": [
    "# 1.2\n",
    "# DO NOT CHANGE THIS CELL\n",
    "!ffprobe -i $TARGET_VIDEO_PATH \\\n",
    "         -hide_banner \\\n",
    "         2>&1| tee my_assessment/video_profile.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "368d5c7b-7798-4920-800f-608d3752d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3\n",
    "FRAME_RATE=59.94\n",
    "FRAME_HEIGHT=720\n",
    "FRAME_WIDTH=1280\n",
    "FRAME_CODEC='yuv'\n",
    "FRAME_COLOR_FORMAT='h264'\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "Answer=f\"\"\"\\\n",
    "FRAME RATE: {round(FRAME_RATE)} FPS \\\n",
    "HEIGHT: {FRAME_HEIGHT} \\\n",
    "WIDTH: {FRAME_WIDTH} \\\n",
    "FRAME_CODEC: {FRAME_CODEC} \\\n",
    "FRAME_COLOR_FORMAT: {FRAME_COLOR_FORMAT} \\\n",
    "\"\"\"\n",
    "\n",
    "!echo $Answer > my_assessment/answer_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998b0e1-2bb8-4922-af6a-710f2e47f9df",
   "metadata": {},
   "source": [
    "### Step 2: Brainstorm AI Inference and Download a Pre-Trained Model ###\n",
    "The next step is to brain storm the AI inference needed to achieve the objective. For this application, we need to detect cars in the frame and identify cases when the bounding box crosses below a threshold (illustrated below). \n",
    "\n",
    "<p><img src='images/tailgating_logic.png' width=720></p>\n",
    "\n",
    "Fortunately, there is a [DashCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/dashcamnet) purpose-built object detection model that has been trained on similar data as our video. We can use the [NGC CLI](https://ngc.nvidia.com/setup/installers/cli) to download the [DashCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/dashcamnet) model for our application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba02d6-91d7-467a-8063-4b161beda2cf",
   "metadata": {},
   "source": [
    "**Instructions**: <br>\n",
    "2.1 Execute the cell to install the NGC CLI. <br>\n",
    "2.2 Execute the cell to use the `ngc registry mode list` command that lists all available models. We use the `--column name`, `--column repository`, and `--column application` options to display only the relevant columns. Afterwards, review the model card for [DashCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/dashcamnet) and confirm that the model is fit for purpose. <br>\n",
    "2.3 Update the `<FIXME>`s _only_ and execute the cell to download the [DashCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/dashcamnet) model. The output of the command will generate a text file for grading purposes. _You can execute this cell multiple times until satisfactory_. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f0c649-280e-4ad9-a45f-4125c36afa13",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLI=ngccli_linux.zip\n",
      "--2025-02-15 08:42:36--  https://ngc.nvidia.com/downloads/ngccli_linux.zip\n",
      "Resolving ngc.nvidia.com (ngc.nvidia.com)... 18.165.83.111, 18.165.83.119, 18.165.83.59, ...\n",
      "Connecting to ngc.nvidia.com (ngc.nvidia.com)|18.165.83.111|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 48777813 (47M) [application/zip]\n",
      "Saving to: ‘/dli/task/ngc_assets/ngccli/ngccli_linux.zip’\n",
      "\n",
      "ngccli_linux.zip    100%[===================>]  46.52M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-02-15 08:42:36 (313 MB/s) - ‘/dli/task/ngc_assets/ngccli/ngccli_linux.zip’ saved [48777813/48777813]\n",
      "\n",
      "Archive:  /dli/task/ngc_assets/ngccli/ngccli_linux.zip\n"
     ]
    }
   ],
   "source": [
    "# 2.1\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import os\n",
    "os.environ['NGC_DIR']='/dli/task/ngc_assets'\n",
    "\n",
    "%env CLI=ngccli_linux.zip\n",
    "!mkdir -p $NGC_DIR/ngccli\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $NGC_DIR/ngccli\n",
    "!unzip -u \"$NGC_DIR/ngccli/$CLI\" \\\n",
    "       -d $NGC_DIR/ngccli/\n",
    "!rm $NGC_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"NGC_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7bf605e-fe17-48a8-b1b6-70f594a83b43",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------+-------------------------+\n",
      "| Name                    | Repository              | Application             |\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "| CitySemSegFormer        | nvidia/tao/citysemsegfo | OTHER                   |\n",
      "|                         | rmer                    |                         |\n",
      "| VehicleMakeNet          | nvidia/tao/vehiclemaken | Classification          |\n",
      "|                         | et                      |                         |\n",
      "| Pre-trained GCViT       | nvidia/tao/pretrained_g | Image Classification    |\n",
      "| ImageNet Classification | cvit_classification_ima |                         |\n",
      "| weights                 | genet                   |                         |\n",
      "| Optical Inspection      | nvidia/tao/optical_insp | Optical Inspection      |\n",
      "|                         | ection                  |                         |\n",
      "| RIVA Punctuation        | nvidia/tao/punctuationc | NVIDIA Riva             |\n",
      "|                         | apitalization_it_it_ber |                         |\n",
      "|                         | t_base                  |                         |\n",
      "| RIVA Punctuation and    | nvidia/tao/punctuationc | Punctuation and         |\n",
      "| Capitalization for      | apitalization_es_us_ber | Capitalization          |\n",
      "| Spanish                 | t_base                  |                         |\n",
      "| Riva ASR French LM      | nvidia/tao/speechtotext | NVIDIA Riva             |\n",
      "|                         | _fr_fr_lm               |                         |\n",
      "| Riva Marblenet Voice    | nvidia/tao/voiceactivit | NVIDIA Riva             |\n",
      "| Activity Detection      | ydetection_marblenet    |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Mandarin                | _zh_cn_conformer        |                         |\n",
      "| Mask2Former             | nvidia/tao/mask2former  | Instance segmentation   |\n",
      "| PeopleNet               | nvidia/tao/peoplenet    | Other                   |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech To Text          |\n",
      "| Arabic                  | _ar_ar_conformer        |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Other                   |\n",
      "| English                 | _en_gb_conformer        |                         |\n",
      "| Multiple 3D CenterPose  | nvidia/tao/multiclass_3 | Object Pose Estimation  |\n",
      "|                         | d_centerpose            |                         |\n",
      "| Speech Synthesis HiFi-  | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "| GAN                     | sis_hifigan             |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Russian - ASR set 1.0   | _ru_ru_conformer        |                         |\n",
      "| Pre-trained FAN based   | nvidia/tao/pretrained_f | Image Classification    |\n",
      "| ImageNet Classification | an_classification_image |                         |\n",
      "| weights                 | net                     |                         |\n",
      "| BEVFusion for 3D Object | nvidia/tao/bevfusion    | Object Detection        |\n",
      "| Detection               |                         | (People)                |\n",
      "| Pre-trained SegFormer   | nvidia/tao/pretrained_s | Semantic Segmentation   |\n",
      "| NvImageNet weights      | egformer_nvimagenet     |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Japanese                | _ja_jp_conformer        |                         |\n",
      "| RIVA Punctuation and    | nvidia/tao/punctuationc | NVIDIA Riva             |\n",
      "| Capitalization for      | apitalization_hi_in_ber |                         |\n",
      "| Hindi                   | t_base                  |                         |\n",
      "| Pre-trained FasterViT   | nvidia/tao/pretrained_f | Image Classification    |\n",
      "| based NVImageNet        | astervit_classification |                         |\n",
      "| Classification weights  | _nvimagenet             |                         |\n",
      "| Visual ChangeNet        | nvidia/tao/visual_chang | Industrial Inspection   |\n",
      "| Classification          | enet_classification     |                         |\n",
      "| BodyPose3DNet           | nvidia/tao/bodypose3dne | Pose Estimation         |\n",
      "|                         | t                       |                         |\n",
      "| Riva ASR German LM      | nvidia/tao/speechtotext | NVIDIA Riva EA          |\n",
      "|                         | _de_de_lm               |                         |\n",
      "| Speech to Text English  | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Citrinet                | _english_citrinet       |                         |\n",
      "| RIVA Citrinet 256 ASR   | nvidia/tao/speechtotext | Speech To Text          |\n",
      "| English                 | _en_us_citrinet256      |                         |\n",
      "| Riva ASR Hindi LM       | nvidia/tao/speechtotext | Speech To Text          |\n",
      "|                         | _hi_in_lm               |                         |\n",
      "| PeopleNet - AMR         | nvidia/tao/peoplenet_am | Object Detection        |\n",
      "|                         | r                       |                         |\n",
      "| Joint Intent and Slot   | nvidia/tao/intentslotcl | Joint Intent And Slot   |\n",
      "| Classification          | assification_misty_engl | Classification          |\n",
      "| DistilBert              | ish_distilbert          |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Italian                 | _it_it_conformer        |                         |\n",
      "| Visual ChangeNet        | nvidia/tao/visual_chang | Industrial Inspection   |\n",
      "| Segmentation -          | enet_segmentation_levir |                         |\n",
      "| (Research-only)         | cd                      |                         |\n",
      "| RIVA Diarizer Neural    | nvidia/tao/diarizer_vad | Speaker Diarization     |\n",
      "| VAD                     | _multilingual_marblenet |                         |\n",
      "| Riva ASR German Inverse | nvidia/tao/inverse_norm | NVIDIA Riva             |\n",
      "| Normalization Grammar   | alization_de_de         |                         |\n",
      "| FoundationPose          | nvidia/tao/foundationpo | 6-DoF object pose       |\n",
      "|                         | se                      | estimation              |\n",
      "| Riva ASR English        | nvidia/tao/inverse_norm | NVIDIA Riva             |\n",
      "| Inverse Normalization   | alization_en_us         |                         |\n",
      "| Grammar                 |                         |                         |\n",
      "| RIVA EnglishUS Hifigan  | nvidia/tao/speechsynthe | NVIDIA Riva             |\n",
      "|                         | sis_en_us_hifigan       |                         |\n",
      "| RIVA Punctuation and    | nvidia/tao/punctuationc | NVIDIA Riva             |\n",
      "| Capitalization for      | apitalization_pt_br_ber |                         |\n",
      "| Brazilian Portuguese    | t_base                  |                         |\n",
      "| TAO Pretrained          | nvidia/tao/pretrained_c | CLASSIFICATION          |\n",
      "| Classification-TF2      | lassification_tf2       |                         |\n",
      "| TAO Pretrained Object   | nvidia/tao/pretrained_o | Other                   |\n",
      "| Detection               | bject_detection         |                         |\n",
      "| DINO                    | nvidia/tao/pretrained_d | Object Detection        |\n",
      "|                         | ino_coco                |                         |\n",
      "| Pre-trained DINO        | nvidia/tao/pretrained_d | Object Detection        |\n",
      "| NvImageNet weights      | ino_nvimagenet          |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Brazilian Portuguese    | _pt_br_conformer        |                         |\n",
      "| RIVA Citrinet-1024 ASR  | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Spanish EMEA            | _es_es_citrinet         |                         |\n",
      "| RIVA Citrinet-1024 ASR  | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Brazilian Portuguese    | _pt_br_citrinet         |                         |\n",
      "| PeopleSemSegnet         | nvidia/tao/peoplesemseg | OTHER                   |\n",
      "|                         | net                     |                         |\n",
      "| Speech Synthesis        | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "| Waveglow                | sis_waveglow            |                         |\n",
      "| VITA                    | nvidia/tao/vita         | Computer Vision         |\n",
      "| RIVA Punctuation for    | nvidia/tao/punctuationc | NVIDIA Riva             |\n",
      "| Arabic                  | apitalization_ar_ar_ber |                         |\n",
      "|                         | t_base                  |                         |\n",
      "| LITA                    | nvidia/tao/lita         | Computer Vision         |\n",
      "| Riva ASR English(en-GB) | nvidia/tao/speechtotext | NVIDIA Riva             |\n",
      "| LM                      | _en_gb_lm               |                         |\n",
      "| TAO Commercial          | nvidia/tao/nvclip_vit   | Other                   |\n",
      "| Pretrained NV-CLIP      |                         |                         |\n",
      "| Model                   |                         |                         |\n",
      "| PCB Defect              | nvidia/tao/pcb_classifi | Optical Inspection      |\n",
      "| Classification          | cation                  |                         |\n",
      "| RIVA Punctuation        | nvidia/tao/punctuationc | Punctuation and         |\n",
      "|                         | apitalization_en_us_ber | Capitalization          |\n",
      "|                         | t_base                  |                         |\n",
      "| Domain Classification   | nvidia/tao/domainclassi | Domain Classification   |\n",
      "| English Bert            | fication_english_bert   |                         |\n",
      "| PeopleSegNet            | nvidia/tao/peoplesegnet | Instance Segmentation   |\n",
      "| Pre-trained GCViT       | nvidia/tao/pretrained_g | Image Classification    |\n",
      "| NVImageNet              | cvit_classification_nvi |                         |\n",
      "| Classification weights  | magenet                 |                         |\n",
      "| VehicleTypeNet          | nvidia/tao/vehicletypen | Classification          |\n",
      "|                         | et                      |                         |\n",
      "| RIVA Citrinet-1024 ASR  | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Korean                  | _ko_kr_citrinet         |                         |\n",
      "| Transfusion for 3D      | nvidia/tao/transfusion  | Point Cloud - Object    |\n",
      "| Object Detection        |                         | Detection               |\n",
      "| Question Answering      | nvidia/tao/questionansw | Question Answering      |\n",
      "| SQUAD2.0 Bert - Large   | ering_squad_english_ber |                         |\n",
      "|                         | tlarge                  |                         |\n",
      "| LPDNet                  | nvidia/tao/lpdnet       | License Plate Detection |\n",
      "| FaceDetect              | nvidia/tao/facenet      | Object Detection        |\n",
      "| EmotionNet              | nvidia/tao/emotionnet   | Emotion Classification  |\n",
      "| Retail Object           | nvidia/tao/retail_objec | Recognition             |\n",
      "| Recognition             | t_recognition           |                         |\n",
      "| DashCamNet              | nvidia/tao/dashcamnet   | Object Detection        |\n",
      "| Named Entity            | nvidia/tao/namedentityr | Named Entity            |\n",
      "| Recognition Bert        | ecognition_english_bert | Recognition             |\n",
      "| Pre-trained DINO        | nvidia/tao/pretrained_d | Object Detection        |\n",
      "| ImageNet weights        | ino_imagenet            |                         |\n",
      "| ReIdentificationNet     | nvidia/tao/reidentifica | ReIdentification        |\n",
      "|                         | tionnet                 |                         |\n",
      "| Pre-trained Segformer - | nvidia/tao/pretrained_s | Semantic Segmentation   |\n",
      "| CityScapes              | egformer_cityscapes     |                         |\n",
      "| Punctuation and         | nvidia/tao/punctuationc | Punctuation and         |\n",
      "| Capitalization Bert     | apitalization_english_b | Capitalization          |\n",
      "|                         | ert                     |                         |\n",
      "| Pre-trained FAN based   | nvidia/tao/pretrained_f | Image Classification    |\n",
      "| NVImageNet              | an_classification_nvima |                         |\n",
      "| Classification weights  | genet                   |                         |\n",
      "| Visual ChangeNet        | nvidia/tao/visual_chang | Industrial Inspection   |\n",
      "| Segmentation - MvTEC    | enet_segmentation_mvtec |                         |\n",
      "| Pre-trained Deformable  | nvidia/tao/pretrained_d | Object Detection        |\n",
      "| DETR ImageNet weights   | eformable_detr_imagenet |                         |\n",
      "| Joint Intent and Slot   | nvidia/tao/intentslotcl | NLP                     |\n",
      "| Classification Misty    | assification_misty_engl |                         |\n",
      "| Bert                    | ish_bert                |                         |\n",
      "| RIVA Citrinet ASR       | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Mandarin                | _zh_cn_citrinet         |                         |\n",
      "| RIVA Punctuation and    | nvidia/tao/punctuationc | NVIDIA Riva             |\n",
      "| Capitalization for      | apitalization_fr_fr_ber |                         |\n",
      "| French                  | t_base                  |                         |\n",
      "| TAO Pretrained Instance | nvidia/tao/pretrained_i | Instance Segmentation   |\n",
      "| Segmentation            | nstance_segmentation    |                         |\n",
      "| Riva ASR Korean LM      | nvidia/tao/speechtotext | NVIDIA Riva             |\n",
      "|                         | _ko_kr_lm               |                         |\n",
      "| Riva ASR English(en-US) | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| LM                      | _en_us_lm               |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | NVIDIA Riva             |\n",
      "| Hindi - ASR set 2.0     | _hi_in_conformer        |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Spanish EMEA            | _es_es_conformer        |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Korean                  | _ko_kr_conformer        |                         |\n",
      "| RIVA Citrinet ASR       | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| German                  | _de_de_citrinet         |                         |\n",
      "| Mask Auto Label         | nvidia/tao/mask_auto_la | Semantic Segmentation   |\n",
      "|                         | bel                     |                         |\n",
      "| RIVA Punctuation and    | nvidia/tao/punctuationc | Punctuation and         |\n",
      "| Capitalization for      | apitalization_de_de_ber | Capitalization          |\n",
      "| German                  | t_base                  |                         |\n",
      "| RIVA EnglishUS          | nvidia/tao/speechsynthe | NVIDIA Riva             |\n",
      "| Fastpitch               | sis_en_us_fastpitch_ipa |                         |\n",
      "| Pre-trained FasterViT   | nvidia/tao/pretrained_f | Image Classification    |\n",
      "| based ImageNet          | astervit_classification |                         |\n",
      "| Classification weights  | _imagenet               |                         |\n",
      "| Question Answering      | nvidia/tao/questionansw | Question Answering      |\n",
      "| SQUAD2.0 Megatron       | ering_squad_english_meg |                         |\n",
      "|                         | atron                   |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| German                  | _de_de_conformer        |                         |\n",
      "| Pre-trained Deformable  | nvidia/tao/pretrained_d | Object Detection        |\n",
      "| DETR NvImageNet weights | eformable_detr_nvimagen |                         |\n",
      "|                         | et                      |                         |\n",
      "| HeartRateNet            | nvidia/tao/heartratenet | HeartRateNet Estimation |\n",
      "| Optical Character       | nvidia/tao/ocdnet       | Optical Character       |\n",
      "| Detection               |                         | Detection               |\n",
      "| Mask Grounding DINO     | nvidia/tao/mask_groundi | Open vocabulary         |\n",
      "|                         | ng_dino                 | instance segmentation   |\n",
      "| Riva ASR Brazilian      | nvidia/tao/speechtotext | NVIDIA Riva             |\n",
      "| Portuguese LM           | _pt_br_lm               |                         |\n",
      "| Riva ASR Mandarin LM    | nvidia/tao/speechtotext | Speech To Text          |\n",
      "|                         | _zh_cn_lm               |                         |\n",
      "| TAO Pretrained Semantic | nvidia/tao/pretrained_s | Semantic Segmentation   |\n",
      "| Segmentation            | emantic_segmentation    |                         |\n",
      "| RIVA EnglishUS          | nvidia/tao/speechsynthe | NVIDIA Riva             |\n",
      "| Fastpitch               | sis_en_us_fastpitch     |                         |\n",
      "| Riva ASR Spanish        | nvidia/tao/inverse_norm | NVIDIA Riva             |\n",
      "| Inverse Normalization   | alization_es_us         |                         |\n",
      "| Grammar                 |                         |                         |\n",
      "| RIVA Punctuation and    | nvidia/tao/punctuationc | NVIDIA Riva             |\n",
      "| Capitalization for      | apitalization_ko_kr_ber |                         |\n",
      "| Korean                  | t_base                  |                         |\n",
      "| RIVA Diarizer Embedding | nvidia/tao/diarizer_tit | Speaker Diarization     |\n",
      "| Extractor               | anet_small              |                         |\n",
      "| Riva TTS English        | nvidia/tao/normalizatio | NVIDIA Riva             |\n",
      "| Normalization Grammar   | n_en_us                 |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Other                   |\n",
      "| Spanish                 | _es_us_conformer        |                         |\n",
      "| FaceDetectIR            | nvidia/tao/facedetectir | Object Detection        |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| French                  | _fr_fr_conformer        |                         |\n",
      "| Visual ChangeNet-Seg    | nvidia/tao/visual_chang | Other                   |\n",
      "| with FM Backbone -      | enet_changesim          |                         |\n",
      "| ChangeSim               |                         |                         |\n",
      "| Riva TTS English US     | nvidia/tao/speechsynthe | NVIDIA Riva EA          |\n",
      "| Auxiliary Files         | sis_en_us_auxiliary_fil |                         |\n",
      "|                         | es                      |                         |\n",
      "| PeopleNet Transformer   | nvidia/tao/peoplenet_tr | Object Detection        |\n",
      "|                         | ansformer               |                         |\n",
      "| Grounding DINO          | nvidia/tao/grounding_di | Open vocabulary object  |\n",
      "|                         | no                      | detection & phrase      |\n",
      "|                         |                         | detection               |\n",
      "| Riva ASR Spanish LM     | nvidia/tao/speechtotext | NVIDIA Riva EA          |\n",
      "|                         | _es_us_lm               |                         |\n",
      "| TAO Toolkit ODISE 1.1   | nvidia/tao/odise        | Semantic Segmentation   |\n",
      "| Pose Classification     | nvidia/tao/poseclassifi | Pose Classification     |\n",
      "|                         | cationnet               |                         |\n",
      "| TAO Commercial          | nvidia/tao/nvdinov2_vit | Other                   |\n",
      "| Pretrained NV-Dinov2    | g                       |                         |\n",
      "| Model                   |                         |                         |\n",
      "| Pre-trained             | nvidia/tao/pretrained_e | Object Detection        |\n",
      "| EfficientDet Model      | fficientdet_tf2_coco    |                         |\n",
      "| trained on COCO         |                         |                         |\n",
      "| Gaze Estimation         | nvidia/tao/gazenet      | Gaze Detection          |\n",
      "| PeopleSemSegformer      | nvidia/tao/peoplesemseg | Semantic Segmentation   |\n",
      "|                         | former                  |                         |\n",
      "| GestureNet              | nvidia/tao/gesturenet   | Gesture Classification  |\n",
      "| Pre-trained SegFormer   | nvidia/tao/pretrained_s | Semantic Segmentation   |\n",
      "| ImageNet weights        | egformer_imagenet       |                         |\n",
      "| Riva ASR English LM     | nvidia/tao/speechtotext | Riva                    |\n",
      "|                         | _english_lm             |                         |\n",
      "| License Plate           | nvidia/tao/lprnet       | Character Recognition   |\n",
      "| Recognition             |                         |                         |\n",
      "| PeopleSemSegNet AMR     | nvidia/tao/peoplesemseg | Other                   |\n",
      "|                         | net_amr                 |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| English(en-US)          | _en_us_conformer        |                         |\n",
      "| TAO Pretrained DINO     | nvidia/tao/dino_with_fm | Object Detection        |\n",
      "| with Foundational Model | _backbone               |                         |\n",
      "| Backbone                |                         |                         |\n",
      "| TAO Pretrained          | nvidia/tao/pretrained_e | OBJECT_DETECTION        |\n",
      "| EfficientDet-TF2        | fficientdet_tf2         |                         |\n",
      "| Question Answering      | nvidia/tao/questionansw | Question Answering      |\n",
      "| SQUAD2.0 Bert           | ering_squad_english_ber |                         |\n",
      "|                         | t                       |                         |\n",
      "| Retail Object Detection | nvidia/tao/retail_objec | Retail Object Detection |\n",
      "|                         | t_detection             |                         |\n",
      "| Facial Landmarks        | nvidia/tao/fpenet       | Fiducial Landmarks      |\n",
      "| Estimation              |                         |                         |\n",
      "| TrafficCamNet           | nvidia/tao/trafficcamne | Object Detection        |\n",
      "|                         | t                       |                         |\n",
      "| CenterPose              | nvidia/tao/centerpose   | Pose Detection          |\n",
      "| Re-Identification       | nvidia/tao/reidentifica | Re-Identification       |\n",
      "| Transformer             | tionnet_transformer     |                         |\n",
      "| SegIC                   | nvidia/tao/segic        | In-context object       |\n",
      "|                         |                         | segmentation &          |\n",
      "|                         |                         | detection               |\n",
      "| Visual ChangeNet -      | nvidia/tao/visual_chang | Industrial Inspection   |\n",
      "| Segmentation            | enet_segmentation_lands |                         |\n",
      "|                         | atscd                   |                         |\n",
      "| Action Recognition Net  | nvidia/tao/actionrecogn | Action Recognition      |\n",
      "|                         | itionnet                |                         |\n",
      "| TAO Pretrained          | nvidia/tao/pretrained_d | Object Detection        |\n",
      "| DetectNet V2            | etectnet_v2             |                         |\n",
      "| Deformable DETR         | nvidia/tao/pretrained_d | Object Detection        |\n",
      "|                         | eformable_detr_coco     |                         |\n",
      "| Pre-trained             | nvidia/tao/pretrained_e | Object Detection        |\n",
      "| EfficientDet NvImageNet | fficientdet_tf2_nvimage |                         |\n",
      "| backbones               | net                     |                         |\n",
      "| Speech Synthesis        | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "| English FastPitch       | sis_english_fastpitch   |                         |\n",
      "| TAO Pretrained          | nvidia/tao/pretrained_e | Object Detection        |\n",
      "| EfficientDet            | fficientdet             |                         |\n",
      "| TAO Pretrained          | nvidia/tao/pretrained_c | Classification          |\n",
      "| Classification          | lassification           |                         |\n",
      "| PeopleNet Transformer   | nvidia/tao/peoplenet_tr | Object Detection        |\n",
      "| v2.0                    | ansformer_v2            |                         |\n",
      "| RIVA Citrinet ASR       | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Russian                 | _ru_ru_citrinet         |                         |\n",
      "| Optical Character       | nvidia/tao/ocrnet       | Optical Character       |\n",
      "| Recognition             |                         | Recognition             |\n",
      "| Joint Intent and Slot   | nvidia/tao/intentslotcl | Joint Intent and Slot   |\n",
      "| Classification Bert     | assification_weather_en | classification          |\n",
      "|                         | glish_bert              |                         |\n",
      "| Pretrained Mask Auto    | nvidia/tao/pretrained_m | Semantic Segmentation   |\n",
      "| Label                   | ask_auto_label          |                         |\n",
      "| RIVA EnglishUS Hifigan  | nvidia/tao/speechsynthe | NVIDIA Riva             |\n",
      "|                         | sis_en_us_hifigan_ipa   |                         |\n",
      "| RIVA Punctuation        | nvidia/tao/punctuationc | NVIDIA Riva             |\n",
      "|                         | apitalization_ja_jp_ber |                         |\n",
      "|                         | t_base                  |                         |\n",
      "| PointPillarNet          | nvidia/tao/pointpillarn | Object Detection        |\n",
      "|                         | et                      |                         |\n",
      "| Riva ASR Russian LM     | nvidia/tao/speechtotext | NVIDIA Riva EA          |\n",
      "|                         | _ru_ru_lm               |                         |\n",
      "| CenterPose - ISAAC Ros  | nvidia/tao/centerpose_r | Pose Detection          |\n",
      "|                         | os                      |                         |\n",
      "| RIVA Citrinet ASR       | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Spanish                 | _es_us_citrinet         |                         |\n",
      "| RIVA Citrinet ASR Hindi | nvidia/tao/speechtotext | NVIDIA Riva             |\n",
      "| (hi-IN) - ASR set 1.0   | _hi_in_citrinet         |                         |\n",
      "| Riva ASR French Inverse | nvidia/tao/inverse_norm | NVIDIA Riva             |\n",
      "| Normalization Grammar   | alization_fr_fr         |                         |\n",
      "| BodyPoseNet             | nvidia/tao/bodyposenet  | OTHER                   |\n",
      "+-------------------------+-------------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 2.2\n",
    "# DO NOT CHANGE THIS CELL\n",
    "!ngc registry model list nvidia/tao/* --column name --column repository --column application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e46b6d8-bc0a-4db6-b0b7-285b8d0a2b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting files to download...\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K  \u001b[90m━━\u001b[0m • \u001b[32m…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0…\u001b[0m • \u001b[31m…\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 3 - Failed: 0\u001b[0m\n",
      "       \u001b[32m…\u001b[0m                   \u001b[31m…\u001b[0m                                                    \n",
      "\u001b[?25h\n",
      "----------------------------------------------------------------------------\n",
      "   Download status: COMPLETED\n",
      "   Downloaded local path model: /dli/task/ngc_assets/dashcamnet_vpruned_v1.0\n",
      "   Total files downloaded: 3\n",
      "   Total transferred: 6.65 MB\n",
      "   Started at: 2025-02-15 08:42:44\n",
      "   Completed at: 2025-02-15 08:42:45\n",
      "   Duration taken: 0s\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2.3\n",
    "!ngc registry model download-version nvidia/tao/dashcamnet:pruned_v1.0 --dest $NGC_DIR \\\n",
    "2>&1| tee my_assessment/answer_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7764a-2126-44a3-b4d7-b18885d923d4",
   "metadata": {},
   "source": [
    "### Step 3: Edit the Inference Configuration File ###\n",
    "The next step is to modify the Gst-nvinfer configuration file that will be used to configure the AI inference plugin. You can create a new text file for this purpose manually and start from scratch or use the [template provided](spec_files/pgie_config_dashcamnet.txt). You can also refer to sample applications and configuration files [here](https://github.com/NVIDIA-AI-IOT/deepstream_python_apps). When creating the configuration file, below are the fields to pay attention to: \n",
    "\n",
    "Following properties are used when using TAO Toolkit models downloaded from NGC: \n",
    "* `tlt-encoded-model` - Pathname of the TAO Toolkit encoded model\n",
    "* `tlt-model-key` - Model load key for the TAO Toolkit encoded model\n",
    "* `labelfile-path` - Pathname of a text file containing the labels for the model\n",
    "* `int8-calib-file` - Pathname of the INT8 calibration file for dynamic range adjustment with an FP32 model (only in INT8)\n",
    "* `uff-input-blob-name` - Name of the input blob in the UFF file\n",
    "* `output-blob-names` - Array of output layer names\n",
    "* `input-dims` - Dimensions of the model as [channel; height; width; input-order] if input-order=0 i.e. NCHW\n",
    "* `net-scale-factor` - Pixel normalization factor _(default=1)_\n",
    "\n",
    "Recommended properties: \n",
    "* `batch-size` - Number of frames to be inferred together in a batch _(default=1)_\n",
    "\n",
    "Mandatory properties for detectors: \n",
    "* `num-detected-classes` - Number of classes detected by the network\n",
    "\n",
    "Optional properties for detectors: \n",
    "* `cluster-mode` - Clustering algorithm to use _(default=0 i.e. Group Rectangles)_\n",
    "* `interval` - Number of consecutive batches to be skipped for inference _(primary mode only | default=0)_\n",
    "\n",
    "Other optional properties: \n",
    "* `network-mode` - Data format to be used for inference _(0=FP32, 1=INT8, 2=FP16 mode | default=0 i.e. FP32)_\n",
    "* `process-mode` - Mode _(primary or secondary)_ in which the plugin is to operate on _(default=1 i.e. primary)_\n",
    "* `model-color-format` - Color format required by the model _(default=0 i.e. RGB)_\n",
    "* `gie-unique-id` - Unique ID to be assigned to the GIE to enable the application and other elements to identify detected bounding boxes and labels _(default=0)_\n",
    "* `model-engine-file` - Pathname of the serialized model engine file\n",
    "* `gpu-id` - Device ID of GPU to use for pre-processing/inference _(dGPU only)_\n",
    "\n",
    "**Note**: The values in the config file are overridden by values set through GObject properties. Another important thing to remember is that the properties recommended are specific to a primary detector, you will need to work on other properties for secondary and/or classifier. You can find most of the information needed on the [model card](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/dashcamnet): \n",
    "\n",
    "<p><img src='images/model_card.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbcf79-4e80-4312-b462-d4aac1ff87e2",
   "metadata": {},
   "source": [
    "**Instructions**: \n",
    "<br>\n",
    "3.1. Open and review the [configuration file](spec_files/pgie_config_dashcamnet.txt). <br>\n",
    "3.2. Update the `<FIXME>`s _only_ in the configuration file with the correct values and **save changes**. Afterwards, make sure in the cell the correct path of the configuration file is referenced and execute the cell to mark your answer. _You can execute this cell multiple times until satisfactory_. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8e1e1e-0e46-49a3-9c34-e3f1bccee885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[property]\n",
      "gpu-id=0\n",
      "net-scale-factor=0.0039215697906911373\n",
      "tlt-model-key=tlt_encode\n",
      "tlt-encoded-model=/dli/task/ngc_assets/dashcamnet_vpruned_v1.0/resnet18_dashcamnet_pruned.etlt\n",
      "labelfile-path=/dli/task/ngc_assets/dashcamnet_vpruned_v1.0/labels.txt\n",
      "int8-calib-file=/dli/task/ngc_assets/dashcamnet_vpruned_v1.0/dashcamnet_int8.txt\n",
      "input-dims=3;720;1280;0\n",
      "uff-input-blob-name=input_1\n",
      "batch-size=1\n",
      "process-mode=1\n",
      "model-color-format=0\n",
      "# 0=FP32, 1=INT8, 2=FP16 mode\n",
      "network-mode=0\n",
      "num-detected-classes=4\n",
      "interval=0\n",
      "gie-unique-id=1\n",
      "output-blob-names=output_bbox/BiasAdd;output_cov/Sigmoid\n",
      "cluster-mode=0\n",
      "'/dli/task/spec_files/pgie_config_dashcamnet.txt' -> 'my_assessment/answer_3.txt'\n"
     ]
    }
   ],
   "source": [
    "# 3.2\n",
    "os.environ['SPEC_FILE']='/dli/task/spec_files/pgie_config_dashcamnet.txt'\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "!cat $SPEC_FILE\n",
    "!cp -v $SPEC_FILE my_assessment/answer_3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a547a77-8d64-4f83-b26d-91c9ee91edf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4: Build and Run DeepStream Pipeline ###\n",
    "Next, it's time to build the pipeline. We're putting the pipeline creation and initiation procedure inside a function so it an be called easily. We also need to implement the probe callback function prior to running the pipeline. We've provided you a functional architecture and framework for this application to follow. Below is the architecture for this pipeline. \n",
    "\n",
    "<p><img src='images/assessment_pipeline.png' width=1080></p>\n",
    "\n",
    "Our logic for determining if a vehicle is tailgating will be based on the coordinates of detected objects' bounding boxes shown below: \n",
    "\n",
    "<p><img src='images/tailgate_metrics.png' width=720></p>\n",
    "\n",
    "While we attached the probe to the _nvdsosd_ plugin, the only requirement is that it has to be after the _nvinfer_ plugin so it contains the AI-infered metadata. Recall that we need to program the probe [callback function](https://en.wikipedia.org/wiki/Callback_(computer_programming)) to provide us a signal when tailgating is potentially occuring. The probe callback function generally follows a boilerplate, to help iterate through the batches, frames, and objects. For more information on how to implement a callback function, please refer to the [GStreamer Probe documentation](https://gstreamer.freedesktop.org/documentation/additional/design/probes.html). \n",
    "\n",
    "<p><img src='images/probe_boiler_plate.png' width=720></p>\n",
    "\n",
    "We want to generate a list that will contain 0s and 1s for each frame to represent if it exhibits tailgating. Therefore there should be as many numbers as there are number of frames in the end. There should _not_ be one number associated with each object detected as it will lead to more than one number associated with each frame. Below is a sample output: \n",
    "\n",
    "<p><img src='images/sample_log.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860bbe9-b937-47af-a305-95c00d36f580",
   "metadata": {},
   "source": [
    "**Instructions**: \n",
    "<br>\n",
    "4.1. Review the pipeline architecture. <br>\n",
    "4.2. Modify the `<FIXME>` _only_ in the cell with the correct code and execute the cell to define the function that will build and run the pipeline. <br>\n",
    "4.3. Modify the `<FIXME>` _only_ in the cell with the correct code and execute the cell to define the probe callback function. <br>\n",
    "4.4. Execute the cell to run the pipeline. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d8b9c-b28b-4d8f-b226-70ca5b76e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2\n",
    "#Import necessary libraries\n",
    "import sys\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import GObject, Gst, GLib\n",
    "from common.bus_call import bus_call\n",
    "import pyds\n",
    "\n",
    "def run(input_file_path):\n",
    "    global inference_output\n",
    "    inference_output=[]\n",
    "    Gst.init(None)\n",
    "\n",
    "    # Create element that will form a pipeline\n",
    "    print(\"Creating Pipeline\")\n",
    "    pipeline=Gst.Pipeline()\n",
    "    \n",
    "    source=Gst.ElementFactory.make(\"filesrc\", \"file-source\")\n",
    "    source.set_property('location', \"data/assessment_stream.h264\")\n",
    "    h264parser=Gst.ElementFactory.make(\"h264parse\", \"h264-parser\")\n",
    "    decoder=Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\")\n",
    "    \n",
    "    streammux=Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\")    \n",
    "    streammux.set_property('width', 1280)\n",
    "    streammux.set_property('height', 720)\n",
    "    streammux.set_property('batch-size', 1)\n",
    "    \n",
    "    pgie=Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\")\n",
    "    pgie.set_property('config-file-path', os.environ['SPEC_FILE'])\n",
    "    \n",
    "    nvvidconv1=Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\")\n",
    "    nvosd=Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\")\n",
    "    nvvidconv2=Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor2\")\n",
    "    capsfilter=Gst.ElementFactory.make(\"capsfilter\", \"capsfilter\")\n",
    "    caps=Gst.Caps.from_string(\"video/x-raw, format=I420\")\n",
    "    capsfilter.set_property(\"caps\", caps)\n",
    "    \n",
    "    encoder=Gst.ElementFactory.make(\"avenc_mpeg4\", \"encoder\")\n",
    "    encoder.set_property(\"bitrate\", 2000000)\n",
    "    \n",
    "    sink=Gst.ElementFactory.make('filesink', 'filesink')\n",
    "    sink.set_property('location', 'output.mpeg4')\n",
    "    sink.set_property(\"sync\", 1)\n",
    "    \n",
    "    # Add the elements to the pipeline\n",
    "    print(\"Adding elements to Pipeline\")\n",
    "    pipeline.add(source)\n",
    "    pipeline.add(h264parser)\n",
    "    pipeline.add(decoder)\n",
    "    pipeline.add(streammux)\n",
    "    pipeline.add(pgie)\n",
    "    pipeline.add(nvvidconv1)\n",
    "    pipeline.add(nvosd)\n",
    "    pipeline.add(nvvidconv2)\n",
    "    pipeline.add(capsfilter)\n",
    "    pipeline.add(encoder)\n",
    "    pipeline.add(sink)\n",
    "\n",
    "    # Link the elements together\n",
    "    print(\"Linking elements in the Pipeline\")\n",
    "    source.link(h264parser)\n",
    "    h264parser.link(decoder)\n",
    "    decoder.get_static_pad('src').link(streammux.get_request_pad(\"sink_0\"))\n",
    "    streammux.link(pgie)\n",
    "    pgie.link(nvvidconv1)\n",
    "    nvvidconv1.link(nvosd)\n",
    "    nvosd.link(nvvidconv2)\n",
    "    nvvidconv2.link(capsfilter)\n",
    "    capsfilter.link(encoder)\n",
    "    encoder.link(sink)\n",
    "    \n",
    "    # Attach probe to OSD sink pad\n",
    "    osdsinkpad=nvosd.get_static_pad(\"sink\")\n",
    "    osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0)\n",
    "\n",
    "    # Create an event loop and feed gstreamer bus mesages to it\n",
    "    loop=GLib.MainLoop()\n",
    "    bus=pipeline.get_bus()\n",
    "    bus.add_signal_watch()\n",
    "    bus.connect(\"message\", bus_call, loop)\n",
    "    \n",
    "    # Start play back and listen to events\n",
    "    print(\"Starting pipeline\")\n",
    "    \n",
    "    pipeline.set_state(Gst.State.PLAYING)\n",
    "    try:\n",
    "        loop.run()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    pipeline.set_state(Gst.State.NULL)\n",
    "    return inference_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3d3a5e2-aa96-463a-b9e5-bdf69b48bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3\n",
    "# Define the Probe Function\n",
    "def osd_sink_pad_buffer_probe(pad, info, u_data):\n",
    "    gst_buffer=info.get_buffer()\n",
    "\n",
    "    # Retrieve batch metadata from the gst_buffer\n",
    "    batch_meta=pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))\n",
    "    l_frame=batch_meta.frame_meta_list\n",
    "    while l_frame is not None:\n",
    "        \n",
    "        # Initially set the tailgate indicator to False for each frame\n",
    "        tailgate=False\n",
    "        try:\n",
    "            frame_meta=pyds.NvDsFrameMeta.cast(l_frame.data)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        frame_number=frame_meta.frame_num\n",
    "        l_obj=frame_meta.obj_meta_list\n",
    "        \n",
    "        # Iterate through each object to check its dimension\n",
    "        while l_obj is not None:\n",
    "            try:\n",
    "                obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data)\n",
    "                \n",
    "                # If the object meet the criteria then set tailgate indicator to True\n",
    "                obj_bottom=obj_meta.rect_params.top+obj_meta.rect_params.height\n",
    "                obj_width=obj_meta.rect_params.width\n",
    "                if (obj_width > FRAME_WIDTH*.3) & (obj_bottom > FRAME_HEIGHT*.9): \n",
    "                    tailgate=True\n",
    "                    \n",
    "            except StopIteration:\n",
    "                break\n",
    "            try: \n",
    "                l_obj=l_obj.next\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        print(f'Analyzing frame {frame_number}', end='\\r')\n",
    "        inference_output.append(str(int(tailgate)))\n",
    "        try:\n",
    "            l_frame=l_frame.next\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return Gst.PadProbeReturn.OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fef2c16-93b6-4c59-abdc-cd4409cba488",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Pipeline\n",
      "Adding elements to Pipeline\n",
      "Linking elements in the Pipeline\n",
      "Starting pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: gst-library-error-quark: Rounding muxer output width to the next multiple of 8: 1288 (5): gstnvstreammux.c(2795): gst_nvstreammux_change_state (): /GstPipeline:pipeline3/GstNvStreamMux:Stream-muxer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End-of-streamme 2405"
     ]
    }
   ],
   "source": [
    "# 4.4\n",
    "tailgate_log=run(input_file_path='/dli/task/data/assessment_stream.h264')\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "with open('/dli/task/my_assessment/answer_4.txt', 'w') as f: \n",
    "    f.write('\\n'.join(tailgate_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92642fe5-6bbe-45eb-9ed7-5d4cce4a8a59",
   "metadata": {},
   "source": [
    "## Step 5: Analyze the Results ##\n",
    "Finally, we can analyze the driving behavior using the log we've collected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11858951-c2bb-4ca1-a921-55f9287fbbd0",
   "metadata": {},
   "source": [
    "**Instructions**: <br>\n",
    "5.1. Execute the cell to import the tailgate log into a Pandas DataFrame. <br>\n",
    "5.2. Execute the cell to plot the occurences of tailgating. <br>\n",
    "5.3. Make sure the output `.mp4` file is being referenced and execute the cell to view the composite with the bounding boxes drawn into the original video. <br>\n",
    "5.4. Execute the cell to calculate the amount of time on average this vehicle spent tailgating. <br>\n",
    "5.5. Modify the `<FIXME>` _only_ to mark your answer. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80436323-269b-4010-8482-167861e93d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   inference\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.1\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('my_assessment/answer_4.txt', names=['inference'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1da127db-2687-41c2-a026-8988a4bbe8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2160x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.2\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df.plot(kind='bar', figsize=(30, 5))\n",
    "plt.xticks(np.arange(0, len(df)+1, FRAME_RATE), np.arange(0, len(df)/FRAME_RATE))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "523d15a0-9374-419a-869b-b353fc35de36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"output_converted.mp4\" controls  width=\"720\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.3\n",
    "# DO NOT CHANGE THIS CELL\n",
    "!ffmpeg -i output.mpeg4 output_converted.mp4 \\\n",
    "        -y \\\n",
    "        -loglevel quiet\n",
    "\n",
    "Video('output_converted.mp4', width=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ae0b43f-23e1-4ab9-8fb7-0fec6ac7caaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.894431\n",
       "1    0.105569\n",
       "Name: inference, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.4\n",
    "# DO NOT CHANGE THIS CELL\n",
    "display(df['inference'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "983d0dc4-9f3c-4396-b4c0-5e52ba303f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: How much time (without the percentage sign, e.g. 5.0) did the vehicle tailgate? \n",
    "Answer='1.0'\n",
    "\n",
    "# EXAMPLE: \n",
    "# Answer='5.0'\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "!echo $Answer > my_assessment/answer_5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c075a-6f39-4483-9035-af9ac9f65a5c",
   "metadata": {},
   "source": [
    "## Grade Your Code ##\n",
    "If you have completed all 5 questions and confirmed the pipeline runs correctly, save changes to the notebook and revisit the webpage where you launched this interactive environment. Click on the \"**ASSESS TASK**\" button as shown in the screenshot below. Doing so will give you credit for this part of the lab that counts towards earning a certificate of competency for the entire course. \n",
    "\n",
    "<p><img src='images/credit.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b3429-e152-4ffb-a09d-2945ab3582bc",
   "metadata": {},
   "source": [
    "### BONUS. Visualizing Frames ###\n",
    "Below we have included some helpful functions that will help you visualize the frames that exhibit tailgating behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0ee86-e27c-48c5-9274-de08cd3aa6fe",
   "metadata": {},
   "source": [
    "**Instructions**: <br>\n",
    "B.1. Execute the cell to extract tailgating frames. <br>\n",
    "B.2. Execute the cell to display randomly selected tailgating frames. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967bf7f-4332-4965-b713-c424e62d0dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.1\n",
    "# DO NOT CHANGE THIS CELL\n",
    "import cv2\n",
    "\n",
    "!mkdir output_images\n",
    "!rm -r output_images/*\n",
    "input_video=cv2.VideoCapture('output_converted.mp4')\n",
    "retVal, im=input_video.read()\n",
    "frameCount=0\n",
    "while retVal:\n",
    "    if frameCount in df[df['inference']==1].index:\n",
    "        cv2.imwrite(\"output_images/frame_%d.jpg\" % frameCount, im)     # save frame as JPEG file      \n",
    "    retVal, im=input_video.read()\n",
    "    print(f'Read a new frame: {frameCount}', end='\\r')\n",
    "    frameCount+=1\n",
    "input_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a87a09-d228-4c79-b831-868ebeb58a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B.2\n",
    "# DO NOT CHANGE THIS CELL\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "\n",
    "def plot_random_samples(frames):\n",
    "    sample_frames = np.random.choice(frames,size=8)\n",
    "    fig=plt.figure(figsize=(30, 8))\n",
    "    columns = 4\n",
    "    rows = 2\n",
    "    i = 1 \n",
    "    for frame_num in sample_frames:\n",
    "        # im = Image.open('{}/images/{}/{}.jpg'.format(config[\"Base_Dest_Folder\"], config[\"Test_Video_ID\"], box[\"frame_no\"]))\n",
    "        im = Image.open(f'output_images/frame_{frame_num}.jpg')\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        i += 1\n",
    "        plt.imshow(np.asarray(im))\n",
    "    plt.show()\n",
    "    \n",
    "plot_random_samples(df[df['inference']==1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7f52a-c910-4b48-89ee-b241320f0697",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"><img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
